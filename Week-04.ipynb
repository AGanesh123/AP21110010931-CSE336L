{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45773daf",
   "metadata": {},
   "source": [
    "### 1. Implement Linear Regression and calculate sum of residual error on the following Datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e3aa1",
   "metadata": {},
   "source": [
    "#### Used Analytic Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a4c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f84ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbc5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_linear_regression(x, y):\n",
    "    # Construct the design matrix X and np.ones is used for create colums\n",
    "    X = np.column_stack((np.ones_like(x), x))\n",
    "    # Calcuating the coeff using the nprmal form(beta=X^TX)^-1*X^Ty\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y # np.linalg.inv is used to inverse resulting squae matrix\n",
    "    # calculate some of square errors\n",
    "    SSE = np.sum((y - X @ beta) ** 2)\n",
    "    # Calculating r-squared value\n",
    "    R_squared = 1 - (SSE / np.sum((y - np.mean(y)) ** 2))\n",
    "    return beta, SSE, R_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1110e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, learning_rate, num_iterations, batch_size=None, stochastic=False, tolerance=1e-4):\n",
    "    beta = np.zeros(x.shape[1])\n",
    "    n = len(y)\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        idx = np.random.choice(n, batch_size, replace=False) if stochastic else slice(None)\n",
    "        x_batch, y_batch = x[idx], y[idx]\n",
    "        \n",
    "        y_pred = np.dot(x_batch, beta)\n",
    "        error = y_batch - y_pred\n",
    "        \n",
    "        gradient = -2 * np.dot(x_batch.T, error) / n\n",
    "        beta -= learning_rate * gradient\n",
    "        \n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            break\n",
    "    \n",
    "    SSE = np.sum((y - np.dot(x, beta)) ** 2)\n",
    "    SST = np.sum((y - np.mean(y)) ** 2)\n",
    "    R_squared = 1 - (SSE / SST)\n",
    "    \n",
    "    return beta, SSE, R_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f15484f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Descent - Full-batch:\n",
      "Coefficients: [1.23275837 1.17027192]\n",
      "Sum Squared Error (SSE): 5.6242800592066855\n",
      "R-squared Value: 0.9525377210193529\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent - Full-batch\n",
    "beta_full_batch, SSE_full_batch, R_squared_full_batch = gradient_descent(np.column_stack((np.ones_like(x), x)), y, learning_rate=0.001, num_iterations=10000)\n",
    "print(\"\\nGradient Descent - Full-batch:\")\n",
    "print(\"Coefficients:\", beta_full_batch)\n",
    "print(\"Sum Squared Error (SSE):\", SSE_full_batch)\n",
    "print(\"R-squared Value:\", R_squared_full_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3f7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Descent - Stochastic:\n",
      "Coefficients: [0.66371023 1.26199779]\n",
      "Sum Squared Error (SSE): 6.574528344960295\n",
      "R-squared Value: 0.944518748143795\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent - Stochastic\n",
    "beta_stochastic, SSE_stochastic, R_squared_stochastic = gradient_descent(np.column_stack((np.ones_like(x), x)), y, learning_rate=0.001, num_iterations=10000, batch_size=1, stochastic=True)\n",
    "print(\"\\nGradient Descent - Stochastic:\")\n",
    "print(\"Coefficients:\", beta_stochastic)\n",
    "print(\"Sum Squared Error (SSE):\", SSE_stochastic)\n",
    "print(\"R-squared Value:\", R_squared_stochastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267eb22",
   "metadata": {},
   "source": [
    "### 2 . Boston Housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63a5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients using Analytic Formulation: [-51950.59625213  71635.43441227]\n",
      "Coefficients using Full-batch Gradient Descent: [-20469.45491225  64759.44491078]\n",
      "Coefficients using Stochastic Gradient Descent: [-48237.76935829  71569.28348572]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Direct inputs\n",
    "housing_data = pd.DataFrame({\n",
    "    'median_income': [3.5, 7.8, 2.9, 6.1, 4.5],\n",
    "    'median_house_value': [200000, 450000, 150000, 380000, 280000]\n",
    "})\n",
    "\n",
    "selected_attribute = 'median_income'\n",
    "X = housing_data[selected_attribute].values.reshape(-1, 1)\n",
    "y = housing_data['median_house_value'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_with_intercept = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_with_intercept = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "theta_analytic = np.linalg.inv(X_train_with_intercept.T.dot(X_train_with_intercept)).dot(X_train_with_intercept.T).dot(y_train)\n",
    "print(\"Coefficients using Analytic Formulation:\", theta_analytic)\n",
    "\n",
    "def full_batch_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(num_iterations):\n",
    "        y_pred = X.dot(theta)\n",
    "        theta -= (1/len(y)) * learning_rate * X.T.dot(y_pred - y)\n",
    "    return theta\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "theta_full_batch = full_batch_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Full-batch Gradient Descent:\", theta_full_batch)\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(num_iterations):\n",
    "        for i in range(len(y)):\n",
    "            rand_index = np.random.randint(0, len(y))\n",
    "            xi = X[rand_index]\n",
    "            yi = y[rand_index]\n",
    "            y_pred = np.dot(xi, theta)\n",
    "            theta -= learning_rate * xi * (y_pred - yi)\n",
    "    return theta\n",
    "\n",
    "theta_stochastic = stochastic_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Stochastic Gradient Descent:\", theta_stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373d4f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE and R-squared value:\n",
      "Analytic Formulation: SSE = 292187641735.03864 , R-squared = -3.7525641141027757\n",
      "Full-batch Gradient Descent: SSE = 247058233541.1697 , R-squared = -3.018513883233079\n",
      "Stochastic Gradient Descent: SSE = 299105759224.98444 , R-squared = -3.86509042330814\n"
     ]
    }
   ],
   "source": [
    "y_pred_analytic = X_test_with_intercept.dot(theta_analytic)\n",
    "y_pred_full_batch = X_test_with_intercept.dot(theta_full_batch)\n",
    "y_pred_stochastic = X_test_with_intercept.dot(theta_stochastic)\n",
    "\n",
    "SSE_analytic = np.sum((y - y_pred_analytic) ** 2)\n",
    "SSE_full_batch = np.sum((y - y_pred_full_batch) ** 2)\n",
    "SSE_stochastic = np.sum((y - y_pred_stochastic) ** 2)\n",
    "\n",
    "mean_y = np.mean(y)\n",
    "SST = np.sum((y - mean_y) ** 2)\n",
    "\n",
    "R_squared_analytic = 1 - (SSE_analytic / SST)\n",
    "R_squared_full_batch = 1 - (SSE_full_batch / SST)\n",
    "R_squared_stochastic = 1 - (SSE_stochastic / SST)\n",
    "\n",
    "print(\"SSE and R-squared value:\")\n",
    "print(\"Analytic Formulation: SSE =\", SSE_analytic, \", R-squared =\", R_squared_analytic)\n",
    "print(\"Full-batch Gradient Descent: SSE =\", SSE_full_batch, \", R-squared =\", R_squared_full_batch)\n",
    "print(\"Stochastic Gradient Descent: SSE =\", SSE_stochastic, \", R-squared =\", R_squared_stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf38702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
